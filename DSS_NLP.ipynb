{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get Fed Statements (text documents)\n",
    "   ↓\n",
    "2. Extract NLP Features (sentiment, tone, complexity, etc.)\n",
    "   ↓\n",
    "3. Calculate Market Reactions (% change in SPY, gold, etc.)\n",
    "   ↓\n",
    "4. Build ML Model (predict market reaction from NLP features)\n",
    "   ↓\n",
    "5. Analyze Results (which linguistic features matter most?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Big Picture\n",
    "\n",
    "You're basically asking: \"Does the way the Fed talks predict how markets move?\"\n",
    "Your hypothesis: It's not just what the Fed decides (rate hike/cut/hold), but how they say it that matters. A hawkish tone might spook markets even with no rate change, while a dovey, reassuring tone might calm markets even with a rate hike.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Pipeline\n",
    "\n",
    "## A. Sentiment Analysis\n",
    "Goal: Measure positive/negative/neutral tone\n",
    "Models to use:\n",
    "- FinBERT (finance-specific BERT model)\n",
    "    - Outputs: positive, negative, neutral scores (0-1)\n",
    "    - Why: Trained on financial text, understands market language\n",
    "\n",
    "- VADER (rule-based sentiment)\n",
    "    - Outputs: compound score (-1 to +1)\n",
    "    - Why: Fast, good baseline, handles intensity modifiers\n",
    "\n",
    "Features extracted:\n",
    "sentiment_finbert_pos, sentiment_finbert_neg, sentiment_finbert_neutral, sentiment_vader_compound\n",
    "\n",
    "## B. Uncertainty/Risk Detection\n",
    "Goal: Measure how uncertain or risky the language is\n",
    "Model to use:\n",
    "- Loughran-McDonald Financial Dictionary (word lists)\n",
    "    - Counts words like \"uncertain\", \"risk\", \"may\", \"could\"\n",
    "    - Outputs: uncertainty score, risk score (count or percentage)\n",
    "\n",
    "Features extracted:\n",
    "uncertainty_count (# of uncertainty words)\n",
    "risk_count (# of risk words)\n",
    "\n",
    "\n",
    "## C. Readability/Complexity\n",
    "Goal: Measure how complex or \"foggy\" the language is\n",
    "Metrics to use:\n",
    "- Flesch Reading Ease (0-100, higher = easier)\n",
    "- Flesch Kincaid Score (0-15+, readability test that measures the complexity of a text on a U.S. school grade level: lower = easier)\n",
    "- Gunning Fog Index (years of education needed)\n",
    "- Coleman-Liau Index (0-14+, Readability formula that estimates the U.S. grade level required to understand a text by using the number of letters per 100 words and the number of sentences per 100 words: lower=easier)\n",
    "- Average sentence length\n",
    "- Lexical diversity (unique words / total words)\n",
    "\n",
    "Features extracted:\n",
    "flesch_score\n",
    "fog_index\n",
    "avg_sentence_length\n",
    "lexical_diversity\n",
    "Why it matters: More complex statements might signal uncertainty or intentional obfuscation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Extraction Pipeline Proposal**\n",
    "\n",
    "Input: Fed statement text\n",
    "  ↓\n",
    "Step 1: Preprocessing\n",
    "  - Tokenization, lowercasing, remove special chars\n",
    "  ↓\n",
    "Step 2: Sentiment Analysis\n",
    "  - Run FinBERT → get pos/neg/neutral scores\n",
    "  - Run VADER → get compound score\n",
    "  ↓\n",
    "Step 3: Uncertainty/Risk Detection\n",
    "  - Count Loughran-McDonald dictionary words\n",
    "  ↓\n",
    "Step 4: Readability Metrics\n",
    "  - Calculate Flesch, Fog Index, etc.\n",
    "  ↓\n",
    "Output: n-dimensional feature vector per Fed date\n",
    "\n",
    "\n",
    "**Output(Y): Market Reaction - Categorical  (Classification)**\n",
    "- `market_reaction` = \"positive\", \"neutral\", or \"negative\"\n",
    "- Example: If SPY goes up >1%, label it \"positive\"\n",
    "\n",
    "**Each row = one Fed announcement**\n",
    "**Columns:**\n",
    "- **Features (X):** finbert_pos, finbert_neg, vader, uncertainty, etc.\n",
    "- **Target (Y):** SPY_change, gold_change, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dimensionality Reduction (PCA)**\n",
    "\n",
    "### **Why might you need it?**\n",
    "\n",
    "If you have **too many features** (say 50+), some might be:\n",
    "- **Redundant:** finbert_positive and vader_compound might measure similar things\n",
    "- **Noisy:** Some features might not matter at all\n",
    "- **Correlated:** Flesch and Fog Index are often highly correlated\n",
    "\n",
    "Dimensionality reduction **compresses** your features while keeping the important information.\n",
    "\n",
    "### **How PCA works (simple explanation):**\n",
    "\n",
    "Imagine you have 10 features. PCA finds **new axes** (principal components) that capture the most variation in your data.\n",
    "\n",
    "**Before PCA:**\n",
    "```\n",
    "X = [finbert_pos, finbert_neg, vader, uncertainty, ...]  # 10 features\n",
    "```\n",
    "\n",
    "**After PCA:**\n",
    "```\n",
    "X_reduced = [PC1, PC2, PC3, PC4, PC5]  # 5 features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
